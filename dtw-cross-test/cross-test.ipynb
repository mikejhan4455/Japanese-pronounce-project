{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/workspace/persistent/Projects/Japanese-pronounce-project/lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "import sys\n",
    "import os\n",
    "import wave\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import JvsDataset\n",
    "import WordTimestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = 'TakaoPGothic'\n",
    "plt.rcParams[\"font.size\"] = '14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading your vosk model '/workspace/persistent/Projects/Japanese-pronounce-project/lib/vosk-model-ja-0.22'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 1 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 2 orphan components.\n",
      "LOG (VoskAPI:Collapse():nnet-utils.cc:1488) Added 1 components, removed 2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /workspace/persistent/Projects/Japanese-pronounce-project/lib/vosk-model-ja-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from /workspace/persistent/Projects/Japanese-pronounce-project/lib/vosk-model-ja-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from /workspace/persistent/Projects/Japanese-pronounce-project/lib/vosk-model-ja-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo /workspace/persistent/Projects/Japanese-pronounce-project/lib/vosk-model-ja-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from /workspace/persistent/Projects/Japanese-pronounce-project/lib/vosk-model-ja-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from /workspace/persistent/Projects/Japanese-pronounce-project/lib/vosk-model-ja-0.22/rescore/G.carpa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/workspace/persistent/Projects/Japanese-pronounce-project/lib/vosk-model-ja-0.22' model was successfully read\n"
     ]
    }
   ],
   "source": [
    "# file_name = \"\"\n",
    "audio_flienames = [\n",
    "    \"/workspace/persistent/japanese project/jvs_ver1/jvs001/parallel100/wav24kHz16bit/VOICEACTRESS100_001.wav\",\n",
    "    \"/workspace/persistent/japanese project/jvs_ver1/jvs002/parallel100/wav24kHz16bit/VOICEACTRESS100_001.wav\",\n",
    "    # \"/workspace/persistent/japanese project/ohayo.wav\",\n",
    "    # \"/workspace/persistent/japanese project/01-01-rec.m4a\",\n",
    "]\n",
    "\n",
    "jvs_dataset = JvsDataset.JvsDataset(root=\"/workspace/persistent/datasets/jvs_ver1\")\n",
    "recognizer = WordTimestamps.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer.recognize(audio_flienames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "speaker_1 = 1\n",
    "speaker_2 = 2\n",
    "parallel = 31\n",
    "\n",
    "r1 = WordTimestamps.RecognizerResult(recognizer.recognize(jvs_dataset.get_audio_file_path(speaker_1, parallel)))\n",
    "r2 = WordTimestamps.RecognizerResult(recognizer.recognize(jvs_dataset.get_audio_file_path(speaker_2, parallel)))\n",
    "\n",
    "# 1 say word1\n",
    "word_1 = 1\n",
    "mfcc_1_word = r1[word_1].word\n",
    "mfcc_1 = WordTimestamps.get_mfcc_of_clip(jvs_dataset.get_audio_file_path(speaker_1, parallel), r1[word_1].start, r1[word_1].end)\n",
    "# 2 say word1\n",
    "mfcc_2_word = r2[word_1].word\n",
    "mfcc_2 = WordTimestamps.get_mfcc_of_clip(jvs_dataset.get_audio_file_path(speaker_2, parallel), r2[word_1].start, r2[word_1].end)\n",
    "# 2 say word2\n",
    "word_2 = 2\n",
    "mfcc_3_word = r2[word_2].word\n",
    "mfcc_3 = WordTimestamps.get_mfcc_of_clip(jvs_dataset.get_audio_file_path(speaker_2, parallel), r2[word_2].start, r2[word_2].end)\n",
    "\n",
    "print(\"speaker1:{} <-> speaker2:{} : distance = {}\".format(mfcc_1_word, mfcc_2_word, WordTimestamps.get_distance_of_mfcc(mfcc_1, mfcc_2)))\n",
    "print(\"speaker1:{} <-> speaker2:{} : distance = {}\".format(mfcc_1_word, mfcc_3_word, WordTimestamps.get_distance_of_mfcc(mfcc_1, mfcc_3)))\n",
    "print(\"speaker2:{} <-> speaker2:{} : distance = {}\".format(mfcc_2_word, mfcc_3_word, WordTimestamps.get_distance_of_mfcc(mfcc_2, mfcc_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ignore UserWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "parallel = 31\n",
    "\n",
    "data = []\n",
    "r_all = []\n",
    "\n",
    "for speaker in range(1, 11):\n",
    "    r = WordTimestamps.RecognizerResult(recognizer.recognize(jvs_dataset.get_audio_file_path(speaker, parallel)))\n",
    "    r_all.append(r)\n",
    "    print(\"speaker {:02d} says parallel {}: len({})\".format(speaker, parallel, len(r.result)), r.transcript)\n",
    "    \n",
    "    # mfccs = pd.DataFrame(np.array([ pd.DataFrame(i.ravel()) for i in get_mfcc_of_result(jvs_dataset.get_audio_file_path(speaker, parallel), r) ]))\n",
    "    mfccs = pd.DataFrame(np.array([ pd.DataFrame(i.ravel()) for i in WordTimestamps.get_audios_of_result(jvs_dataset.get_audio_file_path(speaker, parallel), r) ])) # not using mfcc\n",
    "    data.append(mfccs)\n",
    "\n",
    "# all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(data, speaker_1, speaker_2):\n",
    "    matrix = []\n",
    "    m = []\n",
    "\n",
    "    for i in data[speaker_1][0]:\n",
    "        for j in data[speaker_2][0]:\n",
    "            m.append(WordTimestamps.get_distance_of_mfcc(i.T, j.T))\n",
    "\n",
    "        matrix.append(m)\n",
    "        m = []\n",
    "\n",
    "    df = pd.DataFrame(matrix, index = [ r_all[speaker_1].result[i].word for i in range(r_all[speaker_1].length)],\n",
    "                    columns = [ r_all[speaker_2].result[i].word for i in range(r_all[speaker_2].length)]).astype(int)\n",
    "\n",
    "    plt.figure(figsize = (15,12))\n",
    "    ax = plt.axes()\n",
    "    ax.set_title('Speaker:{}  vs Speaker:{}'.format(speaker_1, speaker_2), fontsize=20)\n",
    "    sn.heatmap(df, annot=True, cmap='Blues', robust=False, fmt='d', linewidths=.5)\n",
    "\n",
    "    # output\n",
    "    # plt.show()\n",
    "    plt.savefig('./heatmaps/{}-{}-heatmap.png'.format(speaker_1, speaker_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(data, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in np.ndindex((10 ,10)):\n",
    "    plot_heatmap(data, i, j)\n",
    "    print('print {}-{}'.format(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
